{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358ad55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "from io import open\n",
    "\n",
    "import matplotlib\n",
    "#if you are running on the gradx/ugradx/ another cluster, \n",
    "#you will need the following line\n",
    "#if you run on a local machine, you can comment it out\n",
    "matplotlib.use('agg') \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477890fe",
   "metadata": {},
   "source": [
    "## Already given code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b77899",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# we are forcing the use of cpu, if you have access to a gpu, you can set the flag to \"cuda\"\n",
    "# make sure you are very careful if you are using a gpu on a shared cluster/grid, \n",
    "# it can be very easy to confict with other people's jobs.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "SOS_token = \"<SOS>\"\n",
    "EOS_token = \"<EOS>\"\n",
    "\n",
    "SOS_index = 0\n",
    "EOS_index = 1\n",
    "MAX_LENGTH = 15\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\" This class handles the mapping between the words and their indicies\n",
    "    \"\"\"\n",
    "    def __init__(self, lang_code):\n",
    "        self.lang_code = lang_code\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_index: SOS_token, EOS_index: EOS_token}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self._add_word(word)\n",
    "\n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "def split_lines(input_file):\n",
    "    \"\"\"split a file like:\n",
    "    first src sentence|||first tgt sentence\n",
    "    second src sentence|||second tgt sentence\n",
    "    into a list of things like\n",
    "    [(\"first src sentence\", \"first tgt sentence\"), \n",
    "     (\"second src sentence\", \"second tgt sentence\")]\n",
    "    \"\"\"\n",
    "    logging.info(\"Reading lines of %s...\", input_file)\n",
    "    # Read the file and split into lines\n",
    "    lines = open(input_file, encoding='utf-8').read().strip().split('\\n')\n",
    "    # Split every line into pairs\n",
    "    pairs = [l.split('|||') for l in lines]\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def make_vocabs(src_lang_code, tgt_lang_code, train_file):\n",
    "    \"\"\" Creates the vocabs for each of the langues based on the training corpus.\n",
    "    \"\"\"\n",
    "    src_vocab = Vocab(src_lang_code)\n",
    "    tgt_vocab = Vocab(tgt_lang_code)\n",
    "\n",
    "    train_pairs = split_lines(train_file)\n",
    "\n",
    "    for pair in train_pairs:\n",
    "        src_vocab.add_sentence(pair[0])\n",
    "        tgt_vocab.add_sentence(pair[1])\n",
    "\n",
    "    logging.info('%s (src) vocab size: %s', src_vocab.lang_code, src_vocab.n_words)\n",
    "    logging.info('%s (tgt) vocab size: %s', tgt_vocab.lang_code, tgt_vocab.n_words)\n",
    "\n",
    "    return src_vocab, tgt_vocab\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def tensor_from_sentence(vocab, sentence):\n",
    "    \"\"\"creates a tensor from a raw sentence\n",
    "    \"\"\"\n",
    "    indexes = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            indexes.append(vocab.word2index[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "            # logging.warn('skipping unknown subword %s. Joint BPE can produces subwords at test time which are not in vocab. As long as this doesnt happen every sentence, this is fine.', word)\n",
    "    indexes.append(EOS_index)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensors_from_pair(src_vocab, tgt_vocab, pair):\n",
    "    \"\"\"creates a tensor from a raw sentence pair\n",
    "    \"\"\"\n",
    "    input_tensor = tensor_from_sentence(src_vocab, pair[0])\n",
    "    target_tensor = tensor_from_sentence(tgt_vocab, pair[1])\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "\n",
    "######################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64658b0c",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4afea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.embed = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "        \n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"the class for the enoder RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \"\"\"Initilize a word embedding and bi-directional LSTM encoder\n",
    "        For this assignment, you should *NOT* use nn.LSTM. \n",
    "        Instead, you should implement the equations yourself.\n",
    "        See, for example, https://en.wikipedia.org/wiki/Long_short-term_memory#LSTM_with_a_forget_gate\n",
    "        You should make your LSTM modular and re-use it in the Decoder.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # raise NotImplementedError\n",
    "        self.input_size = input_size\n",
    "        self.embed = Embedder(self.input_size, self.hidden_size)\n",
    "        \n",
    "        # Get weight matrices\n",
    "        self.Wh = nn.Linear(self.hidden_size, self.input_size) # W = n x m\n",
    "        self.Wz = nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.Wr = nn.Linear(self.hidden_size, self.input_size)\n",
    "        \n",
    "        self.Uh = nn.linear(self.hidden_size, self.hidden_size)\n",
    "        self.Uz = nn.linear(self.hidden_size, self.hidden_size)\n",
    "        self.Ur = nn.linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # return output, hidden\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"runs the forward pass of the encoder\n",
    "        returns the output and the hidden state\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        r_i = self.sigmoid(self.Wr(x) + self.Ur(hidden))\n",
    "        z_i = self.sigmoid(self.Wz(x) + self.Uz(hidden)) # = output\n",
    "        # compute hidden at i\n",
    "        h_i = self.tanh(self.Wh(x) + self.Uh(r_i * hidden)) # = hidden\n",
    "        \n",
    "        return z_i, h_i\n",
    "        # return output, hidden\n",
    "\n",
    "    def get_initial_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2ffcc",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba60a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \"\"\"the class for the decoder \n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, dropout=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "        \"\"\"Initilize your word embedding, decoder LSTM, and weights needed for your attention here\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # raise NotImplementedError\n",
    "        self.embed = Embedder(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1) ## Check dim when rest of code is set up\n",
    "        \n",
    "        #### make sure W's and U's should be different from the ones in the encoder ####\n",
    "        self.Ws = nn.Linear(self.hidden_size, self.input_size) # W = n x m\n",
    "        self.Wz = nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.Wr = nn.Linear(self.hidden_size, self.input_size)\n",
    "        \n",
    "        self.Us = nn.linear(self.hidden_size, self.hidden_size)\n",
    "        self.Uz = nn.linear(self.hidden_size, self.hidden_size)\n",
    "        self.Ur = nn.linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.Cs = nn.Linear(self.hidden_size, 2*self.hidden_size)\n",
    "        self.Cz = nn.Linear(self.hidden_size, 2*self.hidden_size)\n",
    "        self.Cr = nn.Linear(self.hidden_size, 2*self.hidden_size)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"runs the forward pass of the decoder\n",
    "        returns the log_softmax, hidden state, and attn_weights\n",
    "        \n",
    "        Dropout (self.dropout) should be applied to the word embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # raise NotImplementedError\n",
    "        y = self.embed(input)\n",
    "        \n",
    "        # make sure encoder output is correctly used for c_i.\n",
    "        r_i = self.sigmoid(self.Wr(y) + self.Ur(hidden) + self.Cr(encoder_outputs))\n",
    "        \n",
    "        return log_softmax, hidden, attn_weights\n",
    "\n",
    "    def get_initial_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf55c6",
   "metadata": {},
   "source": [
    "# Define general model and initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, dropout=0.1, max_length=MAX_LENGTH):\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size)\n",
    "        self.decoder = AttnDecoderRNN(hidden_size, output_size, dropout=dropout, max_length=MAX_LENGTH)\n",
    "        \n",
    "\n",
    "\n",
    "def initialize_parameters(model):\n",
    "    \n",
    "    print(\"not implemeneted yet, look at page 14 of paper to see what the weights should be initialized as\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4531969",
   "metadata": {},
   "source": [
    "# Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cbf2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, model, criterion, n_iter,lr, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.get_initial_hidden_state()\n",
    "\n",
    "    # make sure the encoder and decoder are in training mode so dropout is applied\n",
    "    model.train()\n",
    "    loss = []\n",
    "    \n",
    "    params = model.params()\n",
    "    \n",
    "    opt = optim.SGD(params=params, lr=lr)\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    # raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    # USE SGD!\n",
    "    \n",
    "\n",
    "    return loss.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, sentence, src_vocab, tgt_vocab, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    runs tranlsation, returns the output and attention\n",
    "    \"\"\"\n",
    "\n",
    "    # switch the encoder and decoder to eval mode so they are not applying dropout\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_sentence(src_vocab, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.get_initial_hidden_state()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_index]], device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_index:\n",
    "                decoded_words.append(EOS_token)\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(tgt_vocab.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Translate (dev/test)set takes in a list of sentences and writes out their transaltes\n",
    "def translate_sentences(encoder, decoder, pairs, src_vocab, tgt_vocab, max_num_sentences=None, max_length=MAX_LENGTH):\n",
    "    output_sentences = []\n",
    "    for pair in pairs[:max_num_sentences]:\n",
    "        output_words, attentions = translate(encoder, decoder, pair[0], src_vocab, tgt_vocab)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        output_sentences.append(output_sentence)\n",
    "    return output_sentences\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# We can translate random sentences  and print out the\n",
    "# input, target, and output to make some subjective quality judgements:\n",
    "#\n",
    "\n",
    "def translate_random_sentence(encoder, decoder, pairs, src_vocab, tgt_vocab, n=1):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = translate(encoder, decoder, pair[0], src_vocab, tgt_vocab)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    \"\"\"visualize the attention mechanism. And save it to a file. \n",
    "    Plots should look roughly like this: https://i.stack.imgur.com/PhtQi.png\n",
    "    You plots should include axis labels and a legend.\n",
    "    you may want to use matplotlib.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def translate_and_show_attention(input_sentence, encoder1, decoder1, src_vocab, tgt_vocab):\n",
    "    output_words, attentions = translate(\n",
    "        encoder1, decoder1, input_sentence, src_vocab, tgt_vocab)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    show_attention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "def clean(strx):\n",
    "    \"\"\"\n",
    "    input: string with bpe, EOS\n",
    "    output: list without bpe, EOS\n",
    "    \"\"\"\n",
    "    return ' '.join(strx.replace('@@ ', '').replace(EOS_token, '').strip().split())\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--hidden_size', default=256, type=int,\n",
    "                    help='hidden size of encoder/decoder, also word vector size')\n",
    "    ap.add_argument('--n_iters', default=100000, type=int,\n",
    "                    help='total number of examples to train on')\n",
    "    ap.add_argument('--print_every', default=5000, type=int,\n",
    "                    help='print loss info every this many training examples')\n",
    "    ap.add_argument('--checkpoint_every', default=10000, type=int,\n",
    "                    help='write out checkpoint every this many training examples')\n",
    "    ap.add_argument('--initial_learning_rate', default=0.001, type=int,\n",
    "                    help='initial learning rate')\n",
    "    ap.add_argument('--src_lang', default='fr',\n",
    "                    help='Source (input) language code, e.g. \"fr\"')\n",
    "    ap.add_argument('--tgt_lang', default='en',\n",
    "                    help='Source (input) language code, e.g. \"en\"')\n",
    "    ap.add_argument('--train_file', default='data/fren.train.bpe',\n",
    "                    help='training file. each line should have a source sentence,' +\n",
    "                         'followed by \"|||\", followed by a target sentence')\n",
    "    ap.add_argument('--dev_file', default='data/fren.dev.bpe',\n",
    "                    help='dev file. each line should have a source sentence,' +\n",
    "                         'followed by \"|||\", followed by a target sentence')\n",
    "    ap.add_argument('--test_file', default='data/fren.test.bpe',\n",
    "                    help='test file. each line should have a source sentence,' +\n",
    "                         'followed by \"|||\", followed by a target sentence' +\n",
    "                         ' (for test, target is ignored)')\n",
    "    ap.add_argument('--out_file', default='out.txt',\n",
    "                    help='output file for test translations')\n",
    "    ap.add_argument('--load_checkpoint', nargs=1,\n",
    "                    help='checkpoint file to start from')\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    # process the training, dev, test files\n",
    "\n",
    "    # Create vocab from training data, or load if checkpointed\n",
    "    # also set iteration \n",
    "    if args.load_checkpoint is not None:\n",
    "        state = torch.load(args.load_checkpoint[0])\n",
    "        iter_num = state['iter_num']\n",
    "        src_vocab = state['src_vocab']\n",
    "        tgt_vocab = state['tgt_vocab']\n",
    "    else:\n",
    "        iter_num = 0\n",
    "        src_vocab, tgt_vocab = make_vocabs(args.src_lang,\n",
    "                                           args.tgt_lang,\n",
    "                                           args.train_file)\n",
    "\n",
    "    encoder = EncoderRNN(src_vocab.n_words, args.hidden_size).to(device)\n",
    "    decoder = AttnDecoderRNN(args.hidden_size, tgt_vocab.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "    # encoder/decoder weights are randomly initilized\n",
    "    # if checkpointed, load saved weights\n",
    "    if args.load_checkpoint is not None:\n",
    "        encoder.load_state_dict(state['enc_state'])\n",
    "        decoder.load_state_dict(state['dec_state'])\n",
    "\n",
    "    # read in datafiles\n",
    "    train_pairs = split_lines(args.train_file)\n",
    "    dev_pairs = split_lines(args.dev_file)\n",
    "    test_pairs = split_lines(args.test_file)\n",
    "\n",
    "    # set up optimization/loss\n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())  # .parameters() returns generator\n",
    "    optimizer = optim.Adam(params, lr=args.initial_learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # optimizer may have state\n",
    "    # if checkpointed, load saved state\n",
    "    if args.load_checkpoint is not None:\n",
    "        optimizer.load_state_dict(state['opt_state'])\n",
    "\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every args.print_every\n",
    "\n",
    "    while iter_num < args.n_iters:\n",
    "        iter_num += 1\n",
    "        training_pair = tensors_from_pair(src_vocab, tgt_vocab, random.choice(train_pairs))\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter_num % args.checkpoint_every == 0:\n",
    "            state = {'iter_num': iter_num,\n",
    "                     'enc_state': encoder.state_dict(),\n",
    "                     'dec_state': decoder.state_dict(),\n",
    "                     'opt_state': optimizer.state_dict(),\n",
    "                     'src_vocab': src_vocab,\n",
    "                     'tgt_vocab': tgt_vocab,\n",
    "                     }\n",
    "            filename = 'state_%010d.pt' % iter_num\n",
    "            torch.save(state, filename)\n",
    "            logging.debug('wrote checkpoint to %s', filename)\n",
    "\n",
    "        if iter_num % args.print_every == 0:\n",
    "            print_loss_avg = print_loss_total / args.print_every\n",
    "            print_loss_total = 0\n",
    "            logging.info('time since start:%s (iter:%d iter/n_iters:%d%%) loss_avg:%.4f',\n",
    "                         time.time() - start,\n",
    "                         iter_num,\n",
    "                         iter_num / args.n_iters * 100,\n",
    "                         print_loss_avg)\n",
    "            # translate from the dev set\n",
    "            translate_random_sentence(encoder, decoder, dev_pairs, src_vocab, tgt_vocab, n=2)\n",
    "            translated_sentences = translate_sentences(encoder, decoder, dev_pairs, src_vocab, tgt_vocab)\n",
    "\n",
    "            references = [[clean(pair[1]).split(), ] for pair in dev_pairs[:len(translated_sentences)]]\n",
    "            candidates = [clean(sent).split() for sent in translated_sentences]\n",
    "            dev_bleu = corpus_bleu(references, candidates)\n",
    "            logging.info('Dev BLEU score: %.2f', dev_bleu)\n",
    "\n",
    "    # translate test set and write to file\n",
    "    translated_sentences = translate_sentences(encoder, decoder, test_pairs, src_vocab, tgt_vocab)\n",
    "    with open(args.out_file, 'wt', encoding='utf-8') as outf:\n",
    "        for sent in translated_sentences:\n",
    "            outf.write(clean(sent) + '\\n')\n",
    "\n",
    "    # Visualizing Attention\n",
    "    translate_and_show_attention(\"on p@@ eu@@ t me faire confiance .\", encoder, decoder, src_vocab, tgt_vocab)\n",
    "    translate_and_show_attention(\"j en suis contente .\", encoder, decoder, src_vocab, tgt_vocab)\n",
    "    translate_and_show_attention(\"vous etes tres genti@@ ls .\", encoder, decoder, src_vocab, tgt_vocab)\n",
    "    translate_and_show_attention(\"c est mon hero@@ s \", encoder, decoder, src_vocab, tgt_vocab)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
